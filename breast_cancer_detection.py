# -*- coding: utf-8 -*-
"""Breast Cancer detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s4vSHPNQFdz7MBDN2m34wjjVJfacPfHJ
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("wasiqaliyasir/breast-cancer-dataset")

print("Path to dataset files:", path)

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import os

files = os.listdir(path)
print(files)

df = pd.read_csv(os.path.join(path, files[0]))
df.head()

df.info()

df.dtypes

df.shape

df.isnull()

df.describe()

df.columns

"""**Data Visulizations**"""

sns.set(style="dark")
plt.figure(figsize=(10, 10))
df.select_dtypes(include=['number']).hist(bins=15, figsize=(30, 25))

plt.figure(figsize=(30, 30))
sns.heatmap(df.select_dtypes(include='number').corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier

# Drop the 'Unnamed: 32' column as it contains only NaN values
df = df.drop(columns=['Unnamed: 32'])

#Label encoding
df['diagnosis'] = LabelEncoder().fit_transform(df['diagnosis'])

#Choosing The Target

X = df.drop(columns=['diagnosis'])
y = df['diagnosis']

#Normalize features / Spliting and scaling

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
#Models
models = {
    "Logistic Regrresion": LogisticRegression(),
    "Descision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Support Vector Machine": SVC(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(),
}

#Train Predict Evulate

for name, model in models.items():
  model.fit(x_train, y_train)
  y_pred = model.predict(x_test)
  acc = accuracy_score(y_test, y_pred) * 100
  print(f"{name} Accuracy: {acc:.2f}%")

"""*Perceptron*"""

from sklearn.linear_model import Perceptron
clf = Perceptron(
    max_iter=1000,
    eta0=0.1,
    random_state=42,
    tol=1e-3,
    shuffle=True,
)

clf.fit(x_train, y_train)
accuracy = clf.score(x_test, y_test) * 100
print(f"Accuracy: {accuracy:.2f}")

"""Tensorflow"""

import tensorflow as tf
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
print(x_train.shape)
print(y_train.shape)

plt.figure(figsize=(10, 10))
for i in range(10):
  plt.subplot(1, 10, i + 1)
  plt.imshow(x_train[i], cmap="gray")
  plt.axis("off")
  plt.title(f"Label: {y_train[i]}")
plt.tight_layout()
plt.show()

"""# Neural Networks"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

# Loading the Data
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalising the data
x_train = x_train / 255.0
x_test = x_test / 255.0

# One hot encode the labels
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

"""# Building The Neural Network"""

model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation="relu"),
    Dense(10, activation="softmax"),
])

"""# Compiling the Model"""

model.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_test, y_test))

test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_acc:.2f}")

"""# Predict

"""

y_pred_probs = model.predict(x_test)
y_pred = y_pred_probs.argmax(axis=1)
y_true = np.argmax(y_test, axis=1)

#Accuracy and loss plot
plt.figure(figsize=(12, 4))

#Accuracy Plot

plt.subplot(1, 2, 1)
plt.plot(history.history["accuracy"], label="Train accuracy")
plt.plot(history.history["val_accuracy"], label='Validation Accuracy')
plt.title("Model Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

#Loss Plot

plt.subplot(1, 2, 2)
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Validation loss")
plt.title("Model Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.tight_layout()
plt.show()

